# Auto Claude Environment Variables
# Copy this file to .env and fill in your values

# =============================================================================
# AUTHENTICATION (REQUIRED)
# =============================================================================
# Auto Claude uses Claude Code OAuth authentication.
# Direct API keys (ANTHROPIC_API_KEY) are NOT supported to prevent silent billing.
#
# Option 1: Run `claude setup-token` to save token to system keychain (recommended)
#           (macOS: Keychain, Windows: Credential Manager, Linux: secret-service)
# Option 2: Set the token explicitly:
# CLAUDE_CODE_OAUTH_TOKEN=your-oauth-token-here
#
# For enterprise/proxy setups (CCR):
# ANTHROPIC_AUTH_TOKEN=sk-zcf-x-ccr

# =============================================================================
# CUSTOM API ENDPOINT (OPTIONAL)
# =============================================================================
# Override the default Anthropic API endpoint. Useful for:
#   - Local proxies (ccr, litellm)
#   - API gateways
#   - Self-hosted Claude instances
#
# ANTHROPIC_BASE_URL=http://127.0.0.1:3456
#
# Related settings (usually set together with ANTHROPIC_BASE_URL):
# NO_PROXY=127.0.0.1
# DISABLE_TELEMETRY=true
# DISABLE_COST_WARNINGS=true
# API_TIMEOUT_MS=600000

# Model override (OPTIONAL)
# Default: claude-opus-4-5-20251101
# AUTO_BUILD_MODEL=claude-opus-4-5-20251101


# =============================================================================
# GIT/WORKTREE SETTINGS (OPTIONAL)
# =============================================================================
# Configure how Auto Claude handles git worktrees for isolated builds.

# --- Three-Tier Branch Strategy (Recommended) ---
# Organize your branches into three tiers:
#   production (stable)  ← you manually merge from feature
#       ↑
#   feature (integration) ← auto-claude merges completed specs here
#       ↑
#   auto-claude/{spec}   ← worktrees for each spec
#
# Benefits:
# - Feature branch collects all auto-claude work
# - You control when changes go to production
# - Clear separation between AI work and stable code
#
# To enable: Create 'feature' and 'production' branches, then set these vars.

# Feature branch (integration branch where completed specs merge)
# Worktrees will be created from this branch.
# FEATURE_BRANCH=feature

# Production branch (stable branch you manually merge to)
# Auto Claude will never push to this branch directly.
# PRODUCTION_BRANCH=production

# --- Legacy Setting ---
# Default base branch for worktree creation (OPTIONAL)
# If FEATURE_BRANCH is not set, falls back to this.
# If neither is set, auto-detects: feature > develop > main > master
# DEFAULT_BRANCH=main

# =============================================================================
# API MODE & MOCK SERVER (OPTIONAL)
# =============================================================================
# Configure API mode for development vs. production environments.
# In development mode, a mock MCP server is used to avoid conflicts with
# production data and enable isolated testing.

# API mode: development | production (default: production)
# Set to 'development' to use mock MCP server for testing
# API_MODE=production

# Mock MCP server port (default: 8001)
# Only used when API_MODE=development or during pytest runs
# MOCK_MCP_PORT=8001

# Graphiti MCP server URL override (OPTIONAL)
# If set, this URL will be used regardless of API_MODE setting.
# This takes precedence over API_MODE and MOCK_MCP_PORT.
# Useful for:
#   - Custom deployments with non-standard ports
#   - Alternative server instances
#   - Testing against specific server configurations
#
# Example: GRAPHITI_MCP_URL=http://localhost:8000/mcp/
# GRAPHITI_MCP_URL=

# =============================================================================
# DEBUG MODE (OPTIONAL)
# =============================================================================
# Enable debug logging for development and troubleshooting.
# Shows detailed information about runner execution, agent calls, file operations.

# Enable debug mode (default: false)
# DEBUG=true

# Debug log level: 1=basic, 2=detailed, 3=verbose (default: 1)
# DEBUG_LEVEL=1

# Log to file instead of stdout (OPTIONAL)
# DEBUG_LOG_FILE=auto-claude/debug.log

# =============================================================================
# LINEAR INTEGRATION (OPTIONAL)
# =============================================================================
# Enable Linear integration for real-time progress tracking in Linear.
# Get your API key from: https://linear.app/YOUR-TEAM/settings/api

# Linear API Key (OPTIONAL - enables Linear integration)
# LINEAR_API_KEY=lin_api_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Pre-configured Team ID (OPTIONAL - will auto-detect if not set)
# LINEAR_TEAM_ID=

# Pre-configured Project ID (OPTIONAL - will create project if not set)
# LINEAR_PROJECT_ID=

# =============================================================================
# JIRA INTEGRATION (OPTIONAL)
# =============================================================================
# Enable Jira integration to import issues as specs, update ticket status,
# and link PRs to Jira issues.
#
# Authentication Options (choose one):
#
#   Option 1: API Token (Simpler)
#   Generate an API token at: https://id.atlassian.com/manage-profile/security/api-tokens
#   Set JIRA_BASE_URL, JIRA_EMAIL, and JIRA_API_TOKEN below.
#
#   Option 2: OAuth 2.0 (More Secure)
#   Create an OAuth app in your Jira instance and set JIRA_BASE_URL,
#   JIRA_OAUTH_CLIENT_ID, and JIRA_OAUTH_CLIENT_SECRET below.
#
# To use:
#   Import a Jira issue: python auto-claude/run.py --jira-import ISSUE-123
#
# Supports both Jira Cloud and Jira Server.

# Jira instance URL (REQUIRED for Jira integration)
# For Cloud: https://your-company.atlassian.net
# For Server: https://jira.your-company.com
# JIRA_BASE_URL=https://your-company.atlassian.net

# --- Option 1: API Token Authentication ---
# Jira email address (REQUIRED for API token auth)
# JIRA_EMAIL=your.email@company.com

# Jira API token (REQUIRED for API token auth)
# Generate at: https://id.atlassian.com/manage-profile/security/api-tokens
# JIRA_API_TOKEN=your-api-token

# --- Option 2: OAuth 2.0 Authentication ---
# OAuth Client ID (REQUIRED for OAuth auth)
# JIRA_OAUTH_CLIENT_ID=your-oauth-client-id

# OAuth Client Secret (REQUIRED for OAuth auth)
# JIRA_OAUTH_CLIENT_SECRET=your-oauth-client-secret

# --- Common Settings ---
# Jira project key (OPTIONAL - default project for filtering)
# Example: ES, PROJ, DEV
# JIRA_PROJECT_KEY=ES

# =============================================================================
# GITLAB INTEGRATION (OPTIONAL)
# =============================================================================
# Enable GitLab integration for issue tracking and merge requests.
# Supports both GitLab.com and self-hosted GitLab instances.
#
# Authentication Options (choose one):
#
#   Option 1: glab CLI OAuth (Recommended)
#   Install glab CLI: https://gitlab.com/gitlab-org/cli#installation
#   Then run: glab auth login
#   This opens your browser for OAuth authentication. Once complete,
#   Auto Claude will automatically use your glab credentials (no env vars needed).
#   For self-hosted: glab auth login --hostname gitlab.example.com
#
#   Option 2: Personal Access Token
#   Set GITLAB_TOKEN below. Token auth is used if set, otherwise falls back to glab CLI.

# GitLab Instance URL (OPTIONAL - defaults to gitlab.com)
# For self-hosted: GITLAB_INSTANCE_URL=https://gitlab.example.com
# GITLAB_INSTANCE_URL=https://gitlab.com

# GitLab Personal Access Token (OPTIONAL - only needed if not using glab CLI)
# Required scope: api (covers issues, merge requests, releases, project info)
# Optional scope: write_repository (only if creating new GitLab projects from local repos)
# Get from: https://gitlab.com/-/user_settings/personal_access_tokens
# GITLAB_TOKEN=glpat-xxxxxxxxxxxxxxxxxxxx

# GitLab Project (OPTIONAL - format: group/project or numeric ID)
# If not set, will auto-detect from git remote
# GITLAB_PROJECT=mygroup/myproject

# =============================================================================
# UI SETTINGS (OPTIONAL)
# =============================================================================
# Enable fancy terminal UI with icons, colors, and interactive menus.
# Set to "false" to use plain text output (useful for CI/CD or log files).

# Enable fancy UI (default: true)
# ENABLE_FANCY_UI=true

# =============================================================================
# ELECTRON MCP SERVER (OPTIONAL)
# =============================================================================
# Enable Electron MCP server for AI agents to interact with and validate
# Electron desktop applications. This allows QA agents to capture screenshots,
# inspect windows, and validate Electron apps during the review process.
#
# The electron-mcp-server connects via Chrome DevTools Protocol to an Electron
# app running with remote debugging enabled.
#
# Prerequisites:
#   1. Start your Electron app with remote debugging:
#      ./YourElectronApp --remote-debugging-port=9222
#
#   2. For auto-claude-ui specifically (use the MCP-enabled scripts):
#      cd auto-claude-ui
#      pnpm run dev:mcp     # Development mode with MCP debugging
#      # OR for production build:
#      pnpm run start:mcp   # Production mode with MCP debugging
#
# Note: Only QA agents (qa_reviewer, qa_fixer) receive Electron MCP tools.
# Coder and Planner agents do NOT have access to these tools to minimize
# context token usage and keep agents focused on their roles.
#
# See: https://github.com/anthropics/anthropic-quickstarts/tree/main/mcp-electron-demo

# Enable Electron MCP integration (default: false)
# ELECTRON_MCP_ENABLED=true

# Chrome DevTools debugging port for Electron connection (default: 9222)
# ELECTRON_DEBUG_PORT=9222

# =============================================================================
# CHROME DEVTOOLS MCP (OPTIONAL)
# =============================================================================
# Enable Chrome DevTools MCP for AI agents to interact with and validate
# web applications through browser automation. This is an alternative to
# Puppeteer MCP that connects directly to Chrome via Chrome DevTools Protocol.
#
# Chrome DevTools MCP provides 26 tools across 6 categories:
#   - Navigation (7): navigate, tabs, history, wait
#   - Input (7): click, fill, hover, drag, dialogs, file upload
#   - Debugging (4): screenshot, DOM snapshot, JavaScript eval, console
#   - Network (2): request listing and details
#
# Prerequisites:
#   1. Start Chrome with remote debugging enabled:
#      Windows: chrome.exe --remote-debugging-port=9222
#      macOS:   /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222
#      Linux:   google-chrome --remote-debugging-port=9222
#
#   2. Chrome 144+ is required for autoConnect feature
#
# When to use:
#   - Testing external React/Vue/Angular web applications
#   - When you need direct Chrome DevTools Protocol access
#   - For web apps that don't work well with Puppeteer's headless mode
#
# Note: Only QA agents (qa_reviewer, qa_fixer) receive Chrome DevTools MCP tools.
# Coder and Planner agents do NOT have access to minimize context token usage.
#
# See: https://github.com/nicholasoxford/chrome-devtools-mcp

# Enable Chrome DevTools MCP integration (default: false)
# CHROME_DEVTOOLS_MCP_ENABLED=true

# =============================================================================
# SONARQUBE INTEGRATION (OPTIONAL)
# =============================================================================
# Enable SonarQube/SonarCloud integration for code quality and security analysis.
# SonarQube provides static code analysis, bug detection, code smells, security
# vulnerabilities, and technical debt tracking.
#
# Setup Options:
#
#   Option 1: SonarCloud (cloud-hosted, free for open-source)
#   - Sign up at: https://sonarcloud.io/
#   - Create a token: https://sonarcloud.io/account/security
#   - Set SONARQUBE_URL=https://sonarcloud.io
#   - Set SONARQUBE_ORGANIZATION to your organization key
#
#   Option 2: Self-hosted SonarQube
#   - Install SonarQube: https://docs.sonarqube.org/latest/setup/install-server/
#   - Create a token in User > My Account > Security
#   - Set SONARQUBE_URL to your server URL

# SonarQube Server URL (OPTIONAL)
# For SonarCloud: https://sonarcloud.io
# For self-hosted: https://sonarqube.example.com
# SONARQUBE_URL=https://sonarcloud.io

# SonarQube Authentication Token (OPTIONAL)
# Generate from: User > My Account > Security > Tokens
# SONARQUBE_TOKEN=squ_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# SonarQube Project Key (OPTIONAL)
# If not set, will auto-detect from project configuration
# Format: typically org_project or namespace:project
# SONARQUBE_PROJECT_KEY=my-org_my-project

# SonarQube Organization (OPTIONAL)
# Required for SonarCloud, optional for self-hosted SonarQube
# SONARQUBE_ORGANIZATION=my-organization

# =============================================================================
# GRAPHITI MEMORY INTEGRATION (REQUIRED)
# =============================================================================
# Graphiti-based persistent memory layer for cross-session context
# retention. Uses LadybugDB as the embedded graph database.
#
# REQUIREMENTS:
#   - Python 3.12 or higher
#   - Install: pip install real_ladybug graphiti-core
#
# Supports multiple LLM and embedder providers:
#   - OpenAI (default)
#   - Anthropic (LLM only, use with Voyage for embeddings)
#   - Azure OpenAI
#   - Ollama (local, fully offline)
#   - Google AI (Gemini)

# Graphiti is enabled by default. Set to false to disable memory features.
GRAPHITI_ENABLED=true

# =============================================================================
# GRAPHITI: Database Settings
# =============================================================================
# LadybugDB stores data in a local directory (no Docker required).

# Database name (default: auto_claude_memory)
# GRAPHITI_DATABASE=auto_claude_memory

# Database storage path (default: ~/.auto-claude/memories)
# GRAPHITI_DB_PATH=~/.auto-claude/memories

# =============================================================================
# GRAPHITI: Provider Selection
# =============================================================================
# Choose which providers to use for LLM and embeddings.
# Default is "openai" for both.

# LLM provider: openai | anthropic | azure_openai | ollama | google | openrouter
# GRAPHITI_LLM_PROVIDER=openai

# Embedder provider: openai | voyage | azure_openai | ollama | google | openrouter
# GRAPHITI_EMBEDDER_PROVIDER=openai

# =============================================================================
# GRAPHITI: OpenAI Provider (Default)
# =============================================================================
# Use OpenAI for both LLM and embeddings. This is the simplest setup.
# Required: OPENAI_API_KEY

# OpenAI API Key
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# OpenAI Model for LLM (default: gpt-4o-mini)
# OPENAI_MODEL=gpt-4o-mini

# OpenAI Model for embeddings (default: text-embedding-3-small)
# Available: text-embedding-3-small (1536 dim), text-embedding-3-large (3072 dim)
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# =============================================================================
# GRAPHITI: Anthropic Provider (LLM only)
# =============================================================================
# Use Anthropic for LLM. Requires separate embedder (use Voyage or OpenAI).
# Example: GRAPHITI_LLM_PROVIDER=anthropic, GRAPHITI_EMBEDDER_PROVIDER=voyage
#
# Required: ANTHROPIC_API_KEY

# Anthropic API Key
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Anthropic Model (default: claude-sonnet-4-5-latest)
# GRAPHITI_ANTHROPIC_MODEL=claude-sonnet-4-5-latest

# =============================================================================
# GRAPHITI: Voyage AI Provider (Embeddings only)
# =============================================================================
# Use Voyage AI for embeddings. Commonly paired with Anthropic LLM.
# Get API key from: https://www.voyageai.com/
#
# Required: VOYAGE_API_KEY

# Voyage AI API Key
# VOYAGE_API_KEY=pa-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Voyage Embedding Model (default: voyage-3)
# Available: voyage-3 (1024 dim), voyage-3-lite (512 dim)
# VOYAGE_EMBEDDING_MODEL=voyage-3

# =============================================================================
# GRAPHITI: Google AI Provider
# =============================================================================
# Use Google AI (Gemini) for both LLM and embeddings.
# Get API key from: https://aistudio.google.com/apikey
#
# Required: GOOGLE_API_KEY

# Google AI API Key
# GOOGLE_API_KEY=AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Google LLM Model (default: gemini-2.0-flash)
# GOOGLE_LLM_MODEL=gemini-2.0-flash

# Google Embedding Model (default: text-embedding-004)
# GOOGLE_EMBEDDING_MODEL=text-embedding-004

# =============================================================================
# GRAPHITI: OpenRouter Provider (Multi-provider aggregator)
# =============================================================================
# Use OpenRouter to access multiple LLM providers through a single API.
# OpenRouter provides access to Anthropic, OpenAI, Google, and many other models.
# Get API key from: https://openrouter.ai/keys
#
# Required: OPENROUTER_API_KEY

# OpenRouter API Key
# OPENROUTER_API_KEY=sk-or-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# OpenRouter Base URL (default: https://openrouter.ai/api/v1)
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# OpenRouter LLM Model (default: anthropic/claude-sonnet-4)
# Popular choices: anthropic/claude-sonnet-4, openai/gpt-4o, google/gemini-2.0-flash
# OPENROUTER_LLM_MODEL=anthropic/claude-sonnet-4

# OpenRouter Embedding Model (default: openai/text-embedding-3-small)
# OPENROUTER_EMBEDDING_MODEL=openai/text-embedding-3-small

# =============================================================================
# GRAPHITI: Azure OpenAI Provider
# =============================================================================
# Use Azure OpenAI for both LLM and embeddings.
# Requires Azure OpenAI deployment with appropriate models.
#
# Required: AZURE_OPENAI_API_KEY, AZURE_OPENAI_BASE_URL

# Azure OpenAI API Key
# AZURE_OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Azure OpenAI Base URL (your Azure endpoint)
# AZURE_OPENAI_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment

# Azure OpenAI Deployment Names
# AZURE_OPENAI_LLM_DEPLOYMENT=gpt-4
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small

# =============================================================================
# GRAPHITI: Ollama Provider (Local/Offline)
# =============================================================================
# Use Ollama for fully offline operation. No API keys required.
# Requires Ollama running locally with appropriate models pulled.
#
# Prerequisites:
#   1. Install Ollama: https://ollama.ai/
#   2. Pull models: ollama pull deepseek-r1:7b && ollama pull nomic-embed-text
#   3. Start Ollama server (usually auto-starts)
#
# Required: OLLAMA_LLM_MODEL, OLLAMA_EMBEDDING_MODEL, OLLAMA_EMBEDDING_DIM

# Ollama Server URL (default: http://localhost:11434)
# OLLAMA_BASE_URL=http://localhost:11434

# Ollama LLM Model
# Popular choices: deepseek-r1:7b, llama3.2:3b, mistral:7b, phi3:medium
# OLLAMA_LLM_MODEL=deepseek-r1:7b

# Ollama Embedding Model
# Popular choices: nomic-embed-text (768 dim), mxbai-embed-large (1024 dim)
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Ollama Embedding Dimension (REQUIRED for Ollama embeddings)
# Must match your embedding model's output dimension
# Common values: nomic-embed-text=768, mxbai-embed-large=1024, all-minilm=384
# OLLAMA_EMBEDDING_DIM=768

# =============================================================================
# GRAPHITI: Example Configurations
# =============================================================================
#
# --- Example 1: OpenAI (simplest) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=openai
# GRAPHITI_EMBEDDER_PROVIDER=openai
# OPENAI_API_KEY=sk-xxxxxxxx
#
# --- Example 2: Anthropic + Voyage (high quality) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=anthropic
# GRAPHITI_EMBEDDER_PROVIDER=voyage
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxx
# VOYAGE_API_KEY=pa-xxxxxxxx
#
# --- Example 3: Ollama (fully offline) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=ollama
# GRAPHITI_EMBEDDER_PROVIDER=ollama
# OLLAMA_LLM_MODEL=deepseek-r1:7b
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# OLLAMA_EMBEDDING_DIM=768
#
# --- Example 4: Azure OpenAI (enterprise) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=azure_openai
# GRAPHITI_EMBEDDER_PROVIDER=azure_openai
# AZURE_OPENAI_API_KEY=xxxxxxxx
# AZURE_OPENAI_BASE_URL=https://your-resource.openai.azure.com/...
# AZURE_OPENAI_LLM_DEPLOYMENT=gpt-4
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small
#
# --- Example 5: Google AI (Gemini) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=google
# GRAPHITI_EMBEDDER_PROVIDER=google
# GOOGLE_API_KEY=AIzaSyxxxxxxxx
#
# --- Example 6: OpenRouter (multi-provider aggregator) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=openrouter
# GRAPHITI_EMBEDDER_PROVIDER=openrouter
# OPENROUTER_API_KEY=sk-or-xxxxxxxx
# OPENROUTER_LLM_MODEL=anthropic/claude-sonnet-4
# OPENROUTER_EMBEDDING_MODEL=openai/text-embedding-3-small
